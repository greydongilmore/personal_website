{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross-validation\n",
    "\n",
    "Splitting our data into training and test sets is a great way to evaluate our model's out-of-sample performance, but it comes at a high cost: it actually increases the propensity to overfit. The reason for this is that we're effectively halving our training sample. As a result, our model has less data to work with, which means it will be more likely to capitalize on chance and fit noise (if this isn't completely intuitive to you yet, I suggest going back to the interactive plot at the end of the previous section and fiddling with it some more).\n",
    "\n",
    "Is there a way to have our cake and eat it too? As it turns out, there is—at least, mostly. The solution is to use a form of cross-validation known as <i>k</i>-fold cross-validation. The idea here is very similar to splitting our data into training and testing halves. In fact, if we set <i>k</i>—a parameter that represents the number of *folds*, or data subsets—to 2, we again end up with two discrete subsets of the data.\n",
    "\n",
    "But now, there's an important twist: instead of using one half of the data for training and the other half for testing, we're going to use both halves for both training and testing. The key is that we'll take turns. First, we'll use Half 1 to train, and Half 2 to test; then, we'll reverse the process. Our final estimate of the model's out-of-sample performance is obtained by averaging the performance estimates we got from the two testing halves. In this way, we've managed to use every single one of our data points for both training and testing, but—critically—never for both at the same time.\n",
    "\n",
    "Of course, we don't have to set <i>k</i> to 2; we can set it to any other value between 2 and the total sample size <i>n</i>. At the limit, if we set <i>k = n</i>, the approach is called  *leave-one-out cross-validation* (because in every fold, we leave out a single data point for testing, and use the rest of the dataset for training). In practice, <i>k</i> is most commonly set to a value in the range of 3 - 10 (there are principled reasons to want to avoid large values of <i>k</i> in many cases, but we won't get into the details here).\n",
    "\n",
    "<div align=\"center\" style=\"font-size: 12px;\"><img src=\"images/kfoldcv.png\" width=\"900\">\n",
    "Image from <a href=\"http://karlrosaen.com/ml/learning-log/2016-06-20/\">http://karlrosaen.com/ml/learning-log/2016-06-20/</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-folds the explicit way\n",
    "To illustrate how k-folds cross-validation works, let's implement it ourselves. First, we create <i>k</i> different subsets of the original dataset. Then, we loop over the <i>k</i> subsets and, in each case, use the current subset to test the model trained on the remaining <i>k</i>-1 subsets. Finally, we average over the performance estimates obtained from all <i>k</i> folds to obtain our overall out-of-sample performance estimate. If you're not interested in wading through the code, you can skip to the next subsection, where we replace most of this with a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training R^2 over folds: 0.68\n",
      "Mean training R^2 over folds: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Number of folds\n",
    "K = 5\n",
    "\n",
    "# initialize results placeholders\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "\n",
    "# our humble steed\n",
    "est = LinearRegression()\n",
    "\n",
    "# create list of indexes and randomize order. if we don't\n",
    "# do this, our folds may be unbalanced if row order is\n",
    "# confounded with other factors.\n",
    "from random import shuffle\n",
    "inds = list(range(len(items)))\n",
    "shuffle(inds)\n",
    "\n",
    "# Loop over the k folds\n",
    "for k in range(K):\n",
    "\n",
    "    # assign every index i to one of k clusters. note that\n",
    "    # the conditional will only pass for (1/k)% of indices\n",
    "    train = [x for (i, x) in enumerate(inds) if i % K != k]\n",
    "\n",
    "    # any indices not in the training set must be in the test\n",
    "    test = list(set(inds) - set(train))\n",
    "\n",
    "    # assign X and y train/test subsets to new variables\n",
    "    X_train, X_test = items.iloc[train], items.iloc[test]\n",
    "    y_train, y_test = age.iloc[train], age.iloc[test]\n",
    "    \n",
    "    # fit the linear regression to only the training data\n",
    "    est.fit(X_train, y_train)\n",
    "    \n",
    "    # compute scores separately for train and test\n",
    "    _train_r2 = est.score(X_train, y_train)\n",
    "    _test_r2 = est.score(X_test, y_test)\n",
    "\n",
    "    # save the R2 scores for this fold\n",
    "    train_r2.append(_train_r2)\n",
    "    test_r2.append(_test_r2)\n",
    "\n",
    "# compute the mean r2 values over all folds\n",
    "train_mean = np.array(train_r2).mean()\n",
    "test_mean = np.array(test_r2).mean()\n",
    "\n",
    "# let's see...\n",
    "print(f\"Mean training R^2 over folds: {train_mean:.2f}\")\n",
    "print(f\"Mean training R^2 over folds: {test_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our out-of-sample predictive performance is now much better than it was before. Why do you think this is?\n",
    "\n",
    "### K-folds the easy way\n",
    "K-folds is an extremely common validation strategy, so any machine learning package worth its salt should provide us with some friendly tools we can use to avoid having to reimplement the basic procedure over and over. In scikit-learn, the `cross_validation` module contains several useful utilities. We've already seen `train_test_split`, which we could use to save us some time. But if all we want to do is get cross-validated scores for some estimator, it's even faster to use the `cross_val_score` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual fold scores: [ 0.3072363   0.17566146  0.2888488  -0.02070723  0.30029865  0.26647716\n",
      "  0.01426081  0.12537567  0.3904625   0.09826141]\n",
      "\n",
      "Mean cross-validated R^2: 0.19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# number of folds\n",
    "K = 10\n",
    "\n",
    "est = LinearRegression()\n",
    "\n",
    "# cross_val_score takes an estimator, our variables, and an\n",
    "# optional specification of the cross-validation procedure.\n",
    "# integers are interpreted as the number of folds to use\n",
    "# in a k-folds partitioning.\n",
    "r2_cv = cross_val_score(est, items, age, cv=K)\n",
    "\n",
    "print(\"Individual fold scores:\", r2_cv)\n",
    "print(f\"\\nMean cross-validated R^2: {r2_cv.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We were able to replace nearly all of our code above with one function call. If you find this a little *too* magical, scikit-learn also has a bunch of other utilities that offer an intermediate level of abstraction (e.g., the `sklearn.model_selection.KFold` class will generate the folds for you, but will return the training and test indices for you to loop over, rather than automatically cross-validating your estimator).\n",
    "\n",
    "Notice that, once again, our cross-validated estimate is a little bit higher than it was in the previous version. The explanation for this improvement is the same as before. (*Hint*: what happens to the training/test allocation as we increase the number of folds? And for bonus points, why might we not automatically want to use the largest value of *k*—i.e., leave-one-out-cross-validation—just because it's likely to give us a more favorable estimate of performance?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
