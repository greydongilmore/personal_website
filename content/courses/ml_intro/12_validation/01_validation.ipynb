{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation using training and test sets\n",
    "\n",
    "An important insight we already introduced in the last section is that an estimator will almost always perform better when evaluated on the same data it was trained on than when evaluated on an entirely new dataset. Since our estimators are usually not much use to us unless they can generalize to new data, we should probably care much more about how an estimator performs on new data than on data it's already seen.\n",
    "\n",
    "The most straightforward way to obtain what's known as an *out-of-sample* performance estimate is to ensure that we always train and evaluate our estimator on independent datasets. The performance estimate obtained from the training dataset will typically suffer from overfitting to some degree; the test dataset estimate will not, so long as its error term is independent of the training dataset.\n",
    "\n",
    "In practice, an easy way to construct training and test datasets with independent errors is to randomly split a dataset in two. We can make use of scikit-learn's `train_test_split` utility, found in the [model selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) module, to do the work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a helpful utility that splits an arbitrary number of\n",
    "# array-like objects into training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get facet scores and age for a \"full\" sample of 1,000\n",
    "items, age = get_features(data, 'items', 'AGE', n=1000)\n",
    "\n",
    "# for every array we pass to train_test_split, we get back\n",
    "# two: a training set, and a test set. the train_size\n",
    "# parameter controls the proportion of all cases assigned\n",
    "# to the training set (the remainder are assigned to test).\n",
    "split_vars = train_test_split(items, age, train_size=0.5)\n",
    "\n",
    "# Python supports parallel assignment: if the number of\n",
    "# variables on the left side matches the number of\n",
    "# elements in a list, the list elements will be mapped\n",
    "# one-to-one onto the variables.\n",
    "items_train, items_test, age_train, age_test = split_vars\n",
    "\n",
    "# Verify shape...\n",
    "items_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit our estimator using the training data, and evaluate its performance using both the training and test data. The difference between the two will tell us how badly we're overfitting to the training data. This practice is called *cross-validation*, and it's ubiquitous in machine learning. In most applications, if you report performance estimates from your training dataset without also reporting a corresponding cross-validated estimate, there's a good chance someone will (not unreasonably) yell at you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 in training sample: 0.79\n",
      "R^2 in test sample: -0.32\n"
     ]
    }
   ],
   "source": [
    "est = LinearRegression()\n",
    "\n",
    "est.fit(items_train, age_train)\n",
    "\n",
    "# Estimate R^2 separately for the training and test samples\n",
    "r2_train = est.score(items_train, age_train)\n",
    "r2_test = est.score(items_test, age_test)\n",
    "\n",
    "print(f\"R^2 in training sample: {round(r2_train, 2)}\")\n",
    "print(f\"R^2 in test sample: {round(r2_test, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here is pretty striking. In the training sample, the fitted model explains a majority of the variance. In the test sample, it explains... well, none. Actually, the value is negative!\n",
    "\n",
    "If you're used to computing $R^2$ by taking the square of a correlation coefficient, you might be thinking that there must be an error somewhere. Correlations range from -1 to 1, and $R^2$ is the square of $R$, so how could we have a negative $R^2$ value?\n",
    "\n",
    "The answer is that the [standard definition of $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) actually allows arbitrarily large negative values, because it's possible for the residual sum-of-squares (RSS) to be larger than the total sum-of-squares (TSS). Intuitively, we can have an estimator that's *so* bad at predicting new scores that we would have been better off just using the mean of the new data as our prediction. In fact, that's exactly what's happening in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
