{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Let's recap some of the general techniques we've discussed for building better predictive models:\n",
    "* Properly validating our models is essential if we want to avoid overfitting and underfitting;\n",
    "* Increasing dataset size is often the single most effective way to improve performance (when feasible); and,\n",
    "* Understanding the inductive biases of different regularization methods can help us pick good estimators.\n",
    "\n",
    "To this toolkit, we now add another broad strategy: picking better features. This is, of course, easier said than done. To a large extent, the process of introducing good features—often referred to as *feature engineering*—is a domain-specific enterprise. This makes it hard to describe general methods for engineering good features: if you don't know anything about personality psychology, for example, you may have trouble deciding which items or scales to include in your surveys.\n",
    "\n",
    "There is, however, one particular approach to feature set construction that can be approached fairly generically: *dimensionality reduction*. In this section, we'll explore strategies for improving predictions by reducing the size of our feature set in various ways.\n",
    "\n",
    "As always, we begin with some imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from support import get_features, plot_learning_curves\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Johnson-2014-IPIP-300.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the tension between dataset size and model complexity that's been a recurring theme throughout this tutorial. As we've seen, when we have a limited amount of data, less complex models often outperform more complex ones, because the former are less likely to overfit.\n",
    "\n",
    "This observation leads directly to another important insight: if we have a feature space that's too big for our model to efficiently learn from, we may be able to reduce the dimensionality of that space until we reach a point that our model is comfortable with. In effect, we're searching for a subspace of our original space that captures most of the reliable variation while shedding most of the noise.\n",
    "\n",
    "As a toy example, suppose we have a dataset containing three personality scales—the facets of `Vulnerability`, `Anxiety`, and `Excitement-Seeking`. When we inspect the data, we observe that two out of the three scales (`Vulnerability` and `Anxiety`) are strongly correlated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Vulnerability</th>\n",
       "      <th>Excitement-Seeking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vulnerability</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Excitement-Seeking</th>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Anxiety  Vulnerability  Excitement-Seeking\n",
       "Anxiety                1.00           0.80               -0.22\n",
       "Vulnerability          0.80           1.00               -0.11\n",
       "Excitement-Seeking    -0.22          -0.11                1.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Anxiety', 'Vulnerability', 'Excitement-Seeking']].corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab5942a4ac4d64a9de6b01a25225f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=45, description='elev', max=135, min=-45), IntSlider(value=45, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact\n",
    "\n",
    "anx, vul, exc = get_features(data, 'Anxiety', 'Vulnerability', 'Excitement-Seeking', n=500)\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_3d_scatter(elev, azim):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(anx, vul, exc, s=20, marker='^')\n",
    "    ax.set(xlabel='Anxiety', ylabel='Vulnerability', zlabel='Excitement-Seeking')\n",
    "    ax.view_init(elev, azim);\n",
    "\n",
    "interact(plot_3d_scatter, elev=45, azim=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two or more features share variance, it's likely (though by no means guaranteed) that they also share predictively useful variance. Consequently, we may be able to reduce the dimensionality of the feature space, or of some subset of the space, without sacrificing much in the way of predictive power.\n",
    "\n",
    "Broadly speaking, there are two approaches we could take. First, we could simply select a subset of existing features, without any further manipulation. Second, we could combine some or all of our features to create a smaller number of derivative features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
